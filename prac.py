# -*- coding: utf-8 -*-
"""prac.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oX5sodp3tPDUm1z1sXR49ioCx-vXVu0D

### Установка всех необходимых библиотек
"""

from IPython.display import clear_output

# Commented out IPython magic to ensure Python compatibility.
# %pip install stanza

clear_output()

# Commented out IPython magic to ensure Python compatibility.
# %pip install bert_score

clear_output()

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers

clear_output()

import tensorflow as tf
import pandas as pd
import numpy as np
import scipy.stats
import tensorflow_hub as hub
from tqdm import tqdm
import stanza
from bert_score import score
from scipy import spatial
import torch
from transformers import BertTokenizer, BertForNextSentencePrediction

stanza.download('ru')
clear_output()

"""### Скачивание текстов, анализ результатов разметки (анализ корреляции), создание таблицы для вычисления признаков"""

marked_paragraphs = pd.read_csv("/content/marked_paragraphs.csv")

marked_paragraphs['familiar_mean'] = marked_paragraphs[['familiar_1', 'familiar_2', 'familiar_3']].mean(axis=1)
marked_paragraphs['understandable_mean'] = marked_paragraphs[['understandable_1', 'understandable_2', 'understandable_3']].mean(axis=1)
marked_paragraphs['word_coherence_mean'] = marked_paragraphs[['word_coherence_1', 'word_coherence_2', 'word_coherence_3']].mean(axis=1)
marked_paragraphs['coherence_mean'] = marked_paragraphs[['coherence_1', 'coherence_2', 'coherence_3']].mean(axis=1)
marked_paragraphs['syntax_compl_mean'] = marked_paragraphs[['syntax_compl_1', 'syntax_compl_2', 'syntax_compl_3']].mean(axis=1)
marked_paragraphs['complexity_mean'] = marked_paragraphs[['complexity_1', 'complexity_2', 'complexity_3']].mean(axis=1)

marked_paragraphs.head()

def compute_correlation(feats, data):
    corr = pd.DataFrame(
        columns=list(feats),
        index=list(feats)
    )
    for feat_1 in feats:
      for feat_2 in feats:
        if feat_1 == feat_2:
          corr_val = 1
        else:
          corr_val = scipy.stats.pearsonr(data[feat_1], data[feat_2])[0]
        corr.at[feat_1, feat_2] = corr_val
    return corr

"""Посчитаем внутриэкспертное соглашение"""

feats = ['familiar_1', 'familiar_2', 'familiar_3', ]
corr_fam = compute_correlation(feats, marked_paragraphs)

feats = ['understandable_1', 'understandable_2', 'understandable_3', ]
corr_und = compute_correlation(feats, marked_paragraphs)

feats = ['word_coherence_1', 'word_coherence_2', 'word_coherence_3', ]
corr_wcoh = compute_correlation(feats, marked_paragraphs)

feats = ['coherence_1', 'coherence_2', 'coherence_3', ]
corr_coh = compute_correlation(feats, marked_paragraphs)

feats = ['syntax_compl_1', 'syntax_compl_2', 'syntax_compl_3', ]
corr_synt = compute_correlation(feats, marked_paragraphs)

feats = ['complexity_1', 'complexity_2', 'complexity_3', ]
corr_compl = compute_correlation(feats, marked_paragraphs)

corr_fam, corr_und, corr_wcoh, corr_coh, corr_synt, corr_compl

feats = ['familiar_mean', 'understandable_mean', 'word_coherence_mean', 'coherence_mean', 'syntax_compl_mean', 'complexity_mean',]
corr = compute_correlation(feats, marked_paragraphs)
corr

import seaborn as sns

#create histogram with density curve overlaid
sns.displot(marked_paragraphs["complexity_mean"], kde=True, bins=15)

feats_table = pd.DataFrame(columns=[
    "paragraph", "complexity_mean", "length_tokens", "noun", "verb", "adj", "adv", "part", "adv_part", "pron",
    "mean_height", "max_height", "mean_leaves", "parts_num",
    "mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs",
])

feats_table["paragraph"] = marked_paragraphs["paragraph"]
feats_table["complexity_mean"] = marked_paragraphs["complexity_mean"]

feats_table["length_tokens"] = marked_paragraphs["paragraph"].map(lambda x: len(x.split()))
print(f"Среднее количество токенов текста: {feats_table['length_tokens'].mean()}")
print(f"Максимальное количество токенов текста: {feats_table['length_tokens'].max()}")
print(f"Минимальное количество токенов текста: {feats_table['length_tokens'].min()}")

# elmo = hub.load("https://tfhub.dev/google/elmo/2")

!wget http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz

import tarfile

with tarfile.open("/content/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz", 'r') as tar:
    tar.extractall("/content/elmo")

clear_output()

elmo = tf.saved_model.load("/content/elmo/")
clear_output()

stanza_proc = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma,depparse')

clear_output()

"""### Функции для вычисления признаков"""

def compute_tree_height(deps, cur_node, height):
  if not deps[cur_node]:
    return height + 1
  else:
    res_height = height
    for node in deps[cur_node]:
      cur_height = compute_tree_height(deps, node, height + 1)
      if cur_height > res_height:
        res_height = cur_height
    return res_height

def compute_syntax_features(paragraph):
  max_height, mean_height, mean_leaves, mean_parts_num = 0, 0, 0, 0
  pos = {"noun": 0, "verb": 0, "adj": 0, "adv": 0, "part": 0, "adv_part": 0, "pron": 0}
  clean_paragraph = []
  for sent in paragraph:
    clean_sent = ''
    doc = stanza_proc(sent)
    deps = {key: [] for key in range(1, len(doc.sentences[0].words) + 1)}
    root = -1
    for word in doc.sentences[0].words:
      if word.deprel == 'root':
        root = word.id
      # print(word)
      if word.head:
        deps[word.head].append(word.id)
      cur_pos = word.upos.lower()
      if cur_pos in pos and cur_pos != "verb":
        pos[cur_pos] += 1
      elif cur_pos == "verb":
        if "VerbForm=Part" in word.feats:
          pos["part"] += 1
        elif "VerbForm=Conv" in word.feats:
          pos["adv_part"] += 1
        else:
          pos["verb"] += 1
      if word.pos.lower() not in {'adp', 'cconj', 'punct', 'intj', 'sym', 'x'}:
        clean_sent += word.text + " "
      if word.deprel == 'nsubj':
        mean_parts_num += 1
    clean_paragraph.append(clean_sent)
    mean_leaves += len([word for word, dep in deps.items() if not dep])
    # print(mean_leaves)
    # print(deps)
    height = compute_tree_height(deps, root, 0)
    # print(height)
    mean_height += height
    if height > max_height:
      max_height = height
  mean_height /= len(paragraph)
  mean_leaves /= len(paragraph)
  mean_parts_num /= len(paragraph)
  return max_height, mean_height, mean_leaves, mean_parts_num, pos, clean_paragraph

def compute_mean_cosine_similarity(paragraph):
  paragraph = [sent.split() for sent in paragraph]
  X = []
  cosines = []
  for sentence in tqdm(paragraph):
    sentence_vec = np.mean(elmo.signatures['default'](tf.convert_to_tensor([sentence])[0])['word_emb'], axis=0)[0]

    X.append(sentence_vec)
  for i in range(1, len(X)):
    cosines.append(1 - spatial.distance.cosine(X[i-1], X[i]))
  return np.mean(cosines)

def compute_mean_bert_score(paragraph):
  scores = []
  for i in range(len(paragraph)):
    p,r,f = score([paragraph[i-1]], [paragraph[i]], lang="ru", verbose=True)
    scores.append(f.numpy()[0])
  return np.mean(scores)

def compute_intersection(paragraph):
  lemmas = []
  intersects = []
  for sent in paragraph:
    doc = stanza_proc(sent)
    for sent in doc.sentences:
      cur_lemmas = []
      for word in sent.words:
        if word.upos != "PUNCT":
          cur_lemmas.append(word.lemma)
      lemmas.append(cur_lemmas)
  for i in range(1, len(lemmas)):
    intersects.append(len(set(lemmas[i-1]) & set(lemmas[i])))
  print(intersects)
  return np.mean(intersects)

# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# bert_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')

tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny')
bert_nsp_model = BertForNextSentencePrediction.from_pretrained('cointegrated/rubert-tiny')

clear_output()

def compute_bert_true_pairs(paragraph):
  true_pairs_num = 0
  for i in range(1, len(paragraph)):
    tokenized = tokenizer(paragraph[i - 1], paragraph[i], return_tensors='pt', truncation=True, max_length=512)
    predict = bert_nsp_model(**tokenized)
    # print(predict, predict.logits)
    logits = predict.logits
    prediction = torch.argmax(predict.logits)
    if prediction == 0:
      true_pairs_num += 1
  return true_pairs_num / (len(paragraph) - 1)

test_par = ["Для обозначения результатов труда программистов обычно использование следующих терминов",
            "Трудом программиста можно считать итоговую программу",
            'Дорога, заросшая высокой травой, вывела нас к полуразрушенному дому и исчезла в густых зарослях.',
              'Это было бледное, крошечное создание, напоминавшее цветок, выросший без лучей солнца.',
            ]
res = compute_bert_true_pairs(test_par)
print(res)

"""### Заполнение таблицы (вычисление признаков)"""

for i in range(len(feats_table)):
  par = feats_table.loc[i]['paragraph'].replace('т.п.', ' тому подобное').replace('т.д.', ' так далее').replace('т.е.', 'то есть')
  par = par.replace('т. п.', ' тому подобное').replace('т. д.', ' так далее').replace('т. е.', 'то есть')
  par = par.replace('др.', 'другой').replace('пр.', 'прочий').replace('!', '.').replace('?', '.').replace('...', '')
  paragraph = [
      sent.strip().replace(',', '').replace('.', '').replace(':', '')
      for sent in par.split(".")
      if sent.strip()
  ]
  sents_num = len(paragraph)
  print(paragraph)
  max_height, mean_height, mean_leaves, mean_parts_num, par_pos, clean_paragraph = compute_syntax_features(paragraph)
  for pos, num in par_pos.items():
    feats_table.at[i, pos] = num / sents_num
  feats_table.at[i, 'max_height'] = max_height
  feats_table.at[i, 'mean_height'] = mean_height
  feats_table.at[i, 'mean_leaves'] = mean_leaves
  feats_table.at[i, 'parts_num'] = mean_parts_num
  print("clean_paragraph = ", clean_paragraph)
  feats_table.at[i, 'mean_cosine_similarity'] = compute_mean_cosine_similarity(clean_paragraph)
  feats_table.at[i, 'mean_bert_score'] = compute_mean_bert_score(clean_paragraph)
  feats_table.at[i, 'mean_intersection'] = compute_intersection(clean_paragraph)
  feats_table.at[i, 'bert_true_pairs'] = compute_bert_true_pairs(clean_paragraph)
  clear_output()

feats_table

feats_table.to_csv("paragraphs_results.csv")

"""### Вычисление корреляции дискурсивных признаков"""

feats_table = pd.read_csv("/content/paragraphs_results.csv")
feats_table["length_tokens"] = feats_table["paragraph"].map(lambda x: len(x.split()))

discourse_feats = feats_table.loc[:, ["mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs"]]
discourse_feats

disc_feats = {"mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs",}
corr = compute_correlation(disc_feats, discourse_feats)
corr

"""### Обучение регрессии"""

feats_table = pd.read_csv("/content/paragraphs_results.csv")
feats_table["length_tokens"] = feats_table["paragraph"].map(lambda x: len(x.split()))

feats_table.head()

from sklearn import preprocessing, svm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

y = np.array(feats_table["complexity_mean"])
X = np.array(feats_table.drop(columns=["paragraph", "complexity_mean", "length_tokens"]))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=30)
regr = LinearRegression()

regr.fit(X_train, y_train)
print(regr.score(X_test, y_test))

y_pred = regr.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Linear Regression mse = ", mse)
print()

feat_names = ["noun", "verb", "adj", "adv", "part", "adv_part", "pron",
    "mean_height", "max_height", "mean_leaves", "parts_num",
    "mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs"]
coefs = regr.coef_
coefs = dict(zip(feat_names, coefs))
for k, v in coefs.items():
  print(f"{k}: {v}")

svr_regr = make_pipeline(StandardScaler(), SVR(kernel='linear'))
parameters = {"svr__C": np.logspace(-2, 1, 10)}
svr_clf = GridSearchCV(svr_regr, parameters, scoring="neg_mean_squared_error")
svr_clf.fit(X_train, y_train)

print(f"Best score for SVR in grid search: {svr_clf.best_score_}")
print(f"Best param: {svr_clf.best_params_}")
svr_regr = svr_clf.best_estimator_

y_pred = svr_regr.predict(X_test)
svr_mse = mean_squared_error(y_test, y_pred)
print("SVR mse = ", svr_mse)
print()

coefs = svr_regr["svr"].coef_[0]
coefs = dict(zip(feat_names, coefs))
for k, v in coefs.items():
  print(f"{k}: {v}")

np.logspace(-2, 1, 10)

elastic_regr = ElasticNet()
parameters = {"alpha": np.logspace(-2, 1, 10), "random_state": [30]}
elastic_clf = GridSearchCV(elastic_regr, parameters, scoring="neg_mean_squared_error")
elastic_clf.fit(X_train, y_train)

print(f"Best score for ElasticNet in grid search: {elastic_clf.best_score_}")
print(f"Best param: {elastic_clf.best_params_}")
elastic_regr = elastic_clf.best_estimator_

y_pred = elastic_regr.predict(X_test)
ridge_mse = mean_squared_error(y_test, y_pred)
print("ElasticNet mse = ", ridge_mse)
print()

coefs = elastic_regr.coef_
coefs = dict(zip(feat_names, coefs))
for k, v in coefs.items():
  print(f"{k}: {v}")

ridge_regr = Ridge()
parameters = {"alpha": np.logspace(-2, 1, 10)}
ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
ridge_clf.fit(X_train, y_train)

print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
print(f"Best param: {ridge_clf.best_params_}")
ridge_regr = ridge_clf.best_estimator_

y_pred = ridge_regr.predict(X_test)
ridge_mse = mean_squared_error(y_test, y_pred)
print("Ridge mse = ", ridge_mse)
print()

coefs = ridge_regr.coef_
coefs = dict(zip(feat_names, coefs))
for k, v in coefs.items():
  print(f"{k}: {v}")

tree_regr = DecisionTreeRegressor()
# parameters = {"alpha": np.logspace(-2, 1, 10)}
# ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
tree_regr.fit(X_train, y_train)

# print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
# print(f"Best param: {ridge_clf.best_params_}")
# ridge_regr = ridge_clf.best_estimator_

y_pred = tree_regr.predict(X_test)
tree_mse = mean_squared_error(y_test, y_pred)
print("Decision Tree mse = ", tree_mse)
print()

coefs = tree_regr.feature_importances_
coefs = dict(zip(feat_names, coefs))
for k, v in coefs.items():
  print(f"{k}: {v}")

forest_regr = RandomForestRegressor()
parameters = {"n_estimators": range(5, 500, 20)}
forest_clf = GridSearchCV(forest_regr, parameters, scoring="neg_mean_squared_error")
forest_clf.fit(X_train, y_train)

print(f"Best score for Random Forest in grid search: {forest_clf.best_score_}")
print(f"Best param: {forest_clf.best_params_}")
forest_regr = forest_clf.best_estimator_

y_pred = forest_regr.predict(X_test)
forest_mse = mean_squared_error(y_test, y_pred)
print("Random forest mse = ", forest_mse)
print()

coefs = forest_regr.feature_importances_
coefs = dict(zip(feat_names, coefs))
for k, v in coefs.items():
  print(f"{k}: {v}")

"""### Сравнение разного набора признаков"""

feats_table['asl'] = np.nan
feats_table['asw'] = np.nan

feats_table.at[0, 'paragraph']

import itertools

for i in range(len(feats_table)):
# for i in range(2):
  par = feats_table.loc[i]['paragraph'].replace('т.п.', ' тому подобное').replace('т.д.', ' так далее').replace('т.е.', 'то есть')
  par = par.replace('т. п.', ' тому подобное').replace('т. д.', ' так далее').replace('т. е.', 'то есть')
  par = par.replace('др.', 'другой').replace('пр.', 'прочий').replace('!', '.').replace('?', '.').replace('...', '')
  paragraph = [
      sent.strip().replace(',', '').replace('.', '').replace(':', '')
      for sent in par.split(".")
      if sent.strip()
  ]
  asl = np.mean([len(sent.split()) for sent in paragraph])
  feats_table.at[i, 'asl'] = asl
  words = list(itertools.chain.from_iterable([sent.split() for sent in paragraph]))
  asw = np.mean([len(word) for word in words])
  feats_table.at[i, 'asw'] = asw

feats_table.head()

y = np.array(feats_table["complexity_mean"])
X = np.array(feats_table[["asl", "asw"]])

X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X, y, test_size = 0.25, random_state=30)
regr = LinearRegression()

regr.fit(X_train_base, y_train_base)
# print(regr.score(X_test, y_test))

y_pred = regr.predict(X_test_base)
mse = mean_squared_error(y_test_base, y_pred)
print("Linear Regression mse = ", mse)
print()

# feat_names = ["noun", "verb", "adj", "adv", "part", "adv_part", "pron",
#     "mean_height", "max_height", "mean_leaves", "parts_num",
#     "mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs"]
# coefs = regr.coef_
# coefs = dict(zip(feat_names, coefs))
# for k, v in coefs.items():
#   print(f"{k}: {v}")

y = feats_table["complexity_mean"]
X = feats_table.drop(columns=["paragraph", "complexity_mean", "length_tokens"])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=30)
regr = LinearRegression()

regr.fit(X_train, y_train)
# print(regr.score(X_test, y_test))

y_pred = regr.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Linear Regression mse = ", mse)
print()

feat_names = ["noun", "verb", "adj", "adv", "part", "adv_part", "pron",
    "mean_height", "max_height", "mean_leaves", "parts_num",
    "mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs", "asl", "asw"]
coefs = regr.coef_
coefs = dict(zip(feat_names, coefs))
sorted_coefs = sorted(coefs.items(), key=lambda x: np.abs(x[1]), reverse=True)
# print(sorted_coefs)
for k, v in sorted_coefs:
  print(f"{k}: {v}")

def compute_ridge_regr(X_train, y_train, X_test, y_test, feat_names):
  ridge_regr = Ridge()
  parameters = {"alpha": np.logspace(-2, 1, 10)}
  ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
  ridge_clf.fit(X_train, y_train)

  print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
  print(f"Best param: {ridge_clf.best_params_}")
  ridge_regr = ridge_clf.best_estimator_

  y_pred = ridge_regr.predict(X_test)
  ridge_mse = mean_squared_error(y_test, y_pred)
  print("Ridge mse = ", ridge_mse)
  print()

  coefs = ridge_regr.coef_
  # print(coefs)
  coefs = dict(zip(feat_names, coefs))
  sorted_coefs = sorted(coefs.items(), key=lambda x: np.abs(x[1]), reverse=True)
  # print(sorted_coefs)
  for k, v in sorted_coefs:
    print(f"{k}: {v}")

compute_ridge_regr(X_train_base, y_train_base, X_test_base, y_test_base, ["asl", "asw"])

# ridge_regr = Ridge()
# parameters = {"alpha": np.logspace(-2, 1, 10)}
# ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
# ridge_clf.fit(X_train_base, y_train_base)

# print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
# print(f"Best param: {ridge_clf.best_params_}")
# ridge_regr = ridge_clf.best_estimator_

# y_pred = ridge_regr.predict(X_test_base)
# ridge_mse = mean_squared_error(y_test_base, y_pred)
# print("Ridge mse = ", ridge_mse)
# print()

# coefs = ridge_regr.coef_
# coefs = dict(zip(feat_names, coefs))
# for k, v in coefs.items():
#   print(f"{k}: {v}")

feat_names = ["noun", "verb", "adj", "adv", "part", "adv_part", "pron",
    "mean_height", "max_height", "mean_leaves", "parts_num",
    "mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs", "asl", "asw"]

compute_ridge_regr(X_train, y_train, X_test, y_test, feat_names)
# ridge_regr = Ridge()
# parameters = {"alpha": np.logspace(-2, 1, 10)}
# ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
# ridge_clf.fit(X_train, y_train)

# print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
# print(f"Best param: {ridge_clf.best_params_}")
# ridge_regr = ridge_clf.best_estimator_

# y_pred = ridge_regr.predict(X_test)
# ridge_mse = mean_squared_error(y_test, y_pred)
# print("Ridge mse = ", ridge_mse)
# print()

# coefs = ridge_regr.coef_
# coefs = dict(zip(feat_names, coefs))
# sorted_coefs = sorted(coefs.items(), key=lambda x: np.abs(x[1]), reverse=True)
# # print(sorted_coefs)
# for k, v in sorted_coefs:
#   print(f"{k}: {v}")

synt_feats = ["noun", "verb", "adj", "adv", "part", "adv_part", "pron",
    "mean_height", "max_height", "mean_leaves", "parts_num"]
y_synt = np.array(feats_table["complexity_mean"])
X_synt = np.array(feats_table[synt_feats])

X_train_synt, X_test_synt, y_train_synt, y_test_synt = train_test_split(X_synt, y_synt, test_size = 0.25, random_state=30)

compute_ridge_regr(X_train_synt, y_train_synt, X_test_synt, y_test_synt, synt_feats)

synt_feats = ["noun", "verb", "adj", "adv", "part", "adv_part", "pron",
    "mean_height", "max_height", "mean_leaves", "parts_num", "asl", "asw"]
y_synt = np.array(feats_table["complexity_mean"])
X_synt = np.array(feats_table[synt_feats])

X_train_synt, X_test_synt, y_train_synt, y_test_synt = train_test_split(X_synt, y_synt, test_size = 0.25, random_state=30)

compute_ridge_regr(X_train_synt, y_train_synt, X_test_synt, y_test_synt, synt_feats)

disc_feats = ["mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs"]
y_disc = np.array(feats_table["complexity_mean"])
X_disc = np.array(feats_table[disc_feats])

X_train_disc, X_test_disc, y_train_disc, y_test_disc = train_test_split(X_disc, y_disc, test_size = 0.25, random_state=30)

compute_ridge_regr(X_train_disc, y_train_disc, X_test_disc, y_test_disc, disc_feats)

disc_feats = ["mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs", "asl", "asw"]
y_disc = np.array(feats_table["complexity_mean"])
X_disc = np.array(feats_table[disc_feats])

X_train_disc, X_test_disc, y_train_disc, y_test_disc = train_test_split(X_disc, y_disc, test_size = 0.25, random_state=30)

compute_ridge_regr(X_train_disc, y_train_disc, X_test_disc, y_test_disc, disc_feats)

print(len(feat_names))
filtered_feats = {name: coef for name, coef in coefs.items() if np.abs(coef) >= 0.05}
print(filtered_feats)
print(len(filtered_feats))
filtered_feats.pop('asl')
filtered_feats.pop('asw')
print(filtered_feats)
combs = set()
for i in range(2, len(filtered_feats.keys())):
  for feat_comb in itertools.combinations(filtered_feats.keys(), i):
    combs.add(feat_comb)
    # print(feat_comb)
print(len(combs))

res = {}
for comb in combs:
  feat_comb = list(comb)
  feat_comb.append('asl')
  feat_comb.append('asw')
  ridge_regr = Ridge(alpha=10)
  ridge_regr.fit(X_train[feat_comb], y_train)

  y_pred = ridge_regr.predict(X_test[feat_comb])
  ridge_mse = mean_squared_error(y_test, y_pred)
  res[tuple(feat_comb)] = ridge_mse

sorted_res = sorted(res.items(), key=lambda x:x[1])
sorted_res[0]

filtered_feats = coefs
filtered_feats.pop('asl')
filtered_feats.pop('asw')
print(filtered_feats)
combs = set()
for i in range(2, len(filtered_feats.keys())):
  for feat_comb in itertools.combinations(filtered_feats.keys(), i):
    combs.add(feat_comb)
    # print(feat_comb)
print(len(combs))

res = {}
for comb in combs:
  feat_comb = list(comb)
  feat_comb.append('asl')
  feat_comb.append('asw')
  ridge_regr = Ridge(alpha=10)
  ridge_regr.fit(X_train[feat_comb], y_train)

  y_pred = ridge_regr.predict(X_test[feat_comb])
  ridge_mse = mean_squared_error(y_test, y_pred)
  res[tuple(feat_comb)] = ridge_mse


sorted_res = sorted(res.items(), key=lambda x:x[1])
sorted_res[0]

"""### Разбиение на батчи по длине текстов"""

import seaborn as sns

#create histogram with density curve overlaid
sns.displot(feats_table["length_tokens"], kde=True, bins=15)

short_paragraphs = feats_table[feats_table["length_tokens"] <= 50]
print(short_paragraphs.shape)
print(len(short_paragraphs))

y_short = short_paragraphs["complexity_mean"]
X_short = short_paragraphs.drop(columns=["paragraph", "complexity_mean", "length_tokens"])
X_train_short, X_test_short, y_train_short, y_test_short = train_test_split(X_short, y_short, test_size = 0.25, random_state=30)

med_paragraphs = feats_table[(feats_table["length_tokens"] > 50) & (feats_table["length_tokens"] <= 85)]
print(len(med_paragraphs))

y_med = med_paragraphs["complexity_mean"]
X_med = med_paragraphs.drop(columns=["paragraph", "complexity_mean", "length_tokens"])
X_train_med, X_test_med, y_train_med, y_test_med = train_test_split(X_med, y_med, test_size = 0.25, random_state=30)

large_paragraphs = feats_table[feats_table["length_tokens"] > 85]
print(len(large_paragraphs))

y_large = large_paragraphs["complexity_mean"]
X_large = large_paragraphs.drop(columns=["paragraph", "complexity_mean", "length_tokens"])
X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(X_large, y_large, test_size = 0.25, random_state=30)

feat_names = ["noun", "verb", "adj", "adv", "part", "adv_part", "pron",
    "mean_height", "max_height", "mean_leaves", "parts_num",
    "mean_cosine_similarity", "mean_bert_score", "mean_intersection", "bert_true_pairs", "asl", "asw"]

X_train_short_base, X_test_short_base = X_train_short[["asl", "asw"]], X_test_short[["asl", "asw"]]

ridge_regr = Ridge()
parameters = {"alpha": np.logspace(-2, 1, 10)}
ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
ridge_clf.fit(X_train_short_base, y_train_short)

print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
print(f"Best param: {ridge_clf.best_params_}")
ridge_regr = ridge_clf.best_estimator_

y_pred = ridge_regr.predict(X_test_short_base)
ridge_mse = mean_squared_error(y_test_short, y_pred)
print("Base res for short paragraphs Ridge mse = ", ridge_mse)
print()

ridge_regr = Ridge()
parameters = {"alpha": np.logspace(-2, 1, 10)}
ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
ridge_clf.fit(X_train_short, y_train_short)

print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
print(f"Best param: {ridge_clf.best_params_}")
ridge_regr = ridge_clf.best_estimator_

y_pred = ridge_regr.predict(X_test_short)
ridge_mse = mean_squared_error(y_test_short, y_pred)
print("short paragraphs Ridge mse = ", ridge_mse)
print()

coefs = ridge_regr.coef_
coefs = dict(zip(feat_names, coefs))
sorted_coefs = sorted(coefs.items(), key=lambda x: np.abs(x[1]), reverse=True)
# print(sorted_coefs)
for k, v in sorted_coefs:
  print(f"{k}: {v}")

X_train_med_base, X_test_med_base = X_train_med[["asl", "asw"]], X_test_med[["asl", "asw"]]

ridge_regr = Ridge()
parameters = {"alpha": np.logspace(-2, 1, 10)}
ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
ridge_clf.fit(X_train_med_base, y_train_med)

print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
print(f"Best param: {ridge_clf.best_params_}")
ridge_regr = ridge_clf.best_estimator_

y_pred = ridge_regr.predict(X_test_med_base)
ridge_mse = mean_squared_error(y_test_med, y_pred)
print("Base res for medium paragraphs Ridge mse = ", ridge_mse)
print()

ridge_regr = Ridge()
parameters = {"alpha": np.logspace(-2, 1, 10)}
ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
ridge_clf.fit(X_train_med, y_train_med)

print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
print(f"Best param: {ridge_clf.best_params_}")
ridge_regr = ridge_clf.best_estimator_

y_pred = ridge_regr.predict(X_test_med)
ridge_mse = mean_squared_error(y_test_med, y_pred)
print("medium paragraphs Ridge mse = ", ridge_mse)
print()

coefs = ridge_regr.coef_
coefs = dict(zip(feat_names, coefs))
sorted_coefs = sorted(coefs.items(), key=lambda x: np.abs(x[1]), reverse=True)
# print(sorted_coefs)
for k, v in sorted_coefs:
  print(f"{k}: {v}")

X_train_large_base, X_test_large_base = X_train_large[["asl", "asw"]], X_test_large[["asl", "asw"]]

ridge_regr = Ridge()
parameters = {"alpha": np.logspace(-2, 1, 10)}
ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
ridge_clf.fit(X_train_large_base, y_train_large)

print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
print(f"Best param: {ridge_clf.best_params_}")
ridge_regr = ridge_clf.best_estimator_

y_pred = ridge_regr.predict(X_test_large_base)
ridge_mse = mean_squared_error(y_test_large, y_pred)
print("Base res for large paragraphs Ridge mse = ", ridge_mse)
print()

ridge_regr = Ridge()
parameters = {"alpha": np.logspace(-2, 1, 10)}
ridge_clf = GridSearchCV(ridge_regr, parameters, scoring="neg_mean_squared_error")
ridge_clf.fit(X_train_large, y_train_large)

print(f"Best score for Ridge in grid search: {ridge_clf.best_score_}")
print(f"Best param: {ridge_clf.best_params_}")
ridge_regr = ridge_clf.best_estimator_

y_pred = ridge_regr.predict(X_test_large)
ridge_mse = mean_squared_error(y_test_large, y_pred)
print("large paragraphs Ridge mse = ", ridge_mse)
print()

coefs = ridge_regr.coef_
coefs = dict(zip(feat_names, coefs))
sorted_coefs = sorted(coefs.items(), key=lambda x: np.abs(x[1]), reverse=True)
# print(sorted_coefs)
for k, v in sorted_coefs:
  print(f"{k}: {v}")

"""### Средние значения признаков"""

feats_1 = feats_table.loc[round(feats_table["complexity_mean"]) == 1].drop("paragraph", axis=1)
print(feats_1.shape)
feats_1.mean()

feats_2 = feats_table.loc[round(feats_table["complexity_mean"]) == 2].drop("paragraph", axis=1)
print(feats_2.shape)
feats_2
feats_2.mean()

feats_3 = feats_table.loc[round(feats_table["complexity_mean"]) == 3].drop("paragraph", axis=1)
print(feats_3.shape)

feats_3.mean()

feats_4 = feats_table.loc[round(feats_table["complexity_mean"]) == 4].drop("paragraph", axis=1)
print(feats_4.shape)

feats_4.mean()

feats_5 = feats_table.loc[round(feats_table["complexity_mean"]) == 5].drop("paragraph", axis=1)
print(feats_5.shape)

feats_5.mean()